{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1. The KNN (K-Nearest Neighbors) algorithm is a simple and intuitive machine learning algorithm used for both classification and regression tasks. It works by finding the K nearest data points in the training set to a given query point and making predictions based on the majority class (for classification) or the average value (for regression) of those nearest neighbors.\n",
    "\n",
    "Q2. The value of K in KNN is typically chosen empirically through techniques like cross-validation. A larger K value smooths the decision boundary, but it may also increase bias, while a smaller K value may lead to a more complex decision boundary and potentially overfitting.\n",
    "\n",
    "Q3. The main difference between the KNN classifier and KNN regressor lies in the type of prediction they make. The KNN classifier predicts the class label of a data point based on the majority class among its K nearest neighbors, while the KNN regressor predicts a continuous value based on the average value of the target variable among its K nearest neighbors.\n",
    "\n",
    "Q4. The performance of KNN can be measured using various metrics depending on the task, such as accuracy, precision, recall, F1-score for classification, and mean squared error, R-squared for regression.\n",
    "\n",
    "Q5. The curse of dimensionality in KNN refers to the phenomenon where the performance of the algorithm deteriorates as the number of dimensions (features) in the data increases. This is because as the number of dimensions increases, the distance between data points becomes less meaningful, making it harder to identify the nearest neighbors accurately.\n",
    "\n",
    "Q6. Missing values in KNN can be handled by imputing them with a value computed from the nearest neighbors. Common methods include replacing missing values with the mean, median, or mode of the nearest neighbors' values.\n",
    "\n",
    "Q7. The performance of KNN classifier and regressor depends on the nature of the problem. KNN classifier is better suited for classification tasks where the decision boundary is non-linear and the classes are well-separated, while KNN regressor is suitable for regression tasks where the relationship between features and target variable is non-linear and there are no clear decision boundaries.\n",
    "\n",
    "Q8.\n",
    "Strengths:\n",
    "\n",
    "Simple and easy to understand.\n",
    "No assumptions about the underlying data distribution.\n",
    "Can be effective for non-linear relationships.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive, especially for large datasets.\n",
    "Sensitive to the choice of K and distance metric.\n",
    "Performs poorly with high-dimensional data due to the curse of dimensionality.\n",
    "These weaknesses can be addressed by using dimensionality reduction techniques, optimizing K through cross-validation, and preprocessing the data to remove irrelevant features or scale the features appropriately.\n",
    "\n",
    "Q9. Euclidean distance and Manhattan distance are two common distance metrics used in KNN:\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space, calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "Manhattan distance, also known as city block distance or L1 distance, is the sum of the absolute differences between corresponding coordinates.\n",
    "In KNN, both distances can be used to measure the distance between data points, but their suitability depends on the nature of the data and the problem.\n",
    "Q10. Feature scaling is essential in KNN because the distance between data points is calculated based on the features' values. If the features have different scales, those with larger scales may dominate the distance calculation. Therefore, it's common practice to scale the features to a similar range, such as normalizing them to have zero mean and unit variance, to ensure all features contribute equally to the distance calculation.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
